{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, gc, sys, warnings, random, math, psutil, pickle\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Helpers\n",
    "#################################################################################\n",
    "## Seeder\n",
    "# :seed to make all processes deterministic     # type: int\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "## Simple \"Memory profilers\" to see memory usage\n",
    "def get_memory_usage():\n",
    "    return np.round(psutil.Process(os.getpid()).memory_info()[0]/2.**30, 2) \n",
    "        \n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Vars\n",
    "#################################################################################\n",
    "SEED = 42\n",
    "LOCAl_TEST = False\n",
    "seed_everything(SEED)\n",
    "TARGET = 'meter_reading'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Data\n"
     ]
    }
   ],
   "source": [
    "########################### DATA LOAD\n",
    "#################################################################################\n",
    "print('Load Data')\n",
    "train_df = pd.read_pickle('../output/ashrae-data-minification/train.pkl')\n",
    "test_df = pd.read_pickle('../output/ashrae-data-minification/test.pkl')\n",
    "\n",
    "building_df = pd.read_pickle('../output/ashrae-data-minification/building_metadata.pkl')\n",
    "\n",
    "train_weather_df = pd.read_pickle('../output/ashrae-data-minification/weather_train.pkl')\n",
    "test_weather_df = pd.read_pickle('../output/ashrae-data-minification/weather_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Building DF merge through concat \n",
    "#################################################################################\n",
    "# Benefits of concat:\n",
    "## Faster for huge datasets (columns number)\n",
    "## No dtype change for dataset\n",
    "## Consume less memmory \n",
    "\n",
    "temp_df = train_df[['building_id']]\n",
    "temp_df = temp_df.merge(building_df, on=['building_id'], how='left')\n",
    "del temp_df['building_id']\n",
    "train_df = pd.concat([train_df, temp_df], axis=1)\n",
    "\n",
    "temp_df = test_df[['building_id']]\n",
    "temp_df = temp_df.merge(building_df, on=['building_id'], how='left')\n",
    "del temp_df['building_id']\n",
    "test_df = pd.concat([test_df, temp_df], axis=1)\n",
    "\n",
    "del building_df, temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Weather DF merge over concat (to not lose type)\n",
    "#################################################################################\n",
    "# Benefits of concat:\n",
    "## Faster for huge datasets (columns number)\n",
    "## No dtype change for dataset\n",
    "## Consume less memmory \n",
    "\n",
    "temp_df = train_df[['site_id','timestamp']]\n",
    "temp_df = temp_df.merge(train_weather_df, on=['site_id','timestamp'], how='left')\n",
    "del temp_df['site_id'], temp_df['timestamp']\n",
    "train_df = pd.concat([train_df, temp_df], axis=1)\n",
    "\n",
    "temp_df = test_df[['site_id','timestamp']]\n",
    "temp_df = temp_df.merge(test_weather_df, on=['site_id','timestamp'], how='left')\n",
    "del temp_df['site_id'], temp_df['timestamp']\n",
    "test_df = pd.concat([test_df, temp_df], axis=1)\n",
    "\n",
    "del train_weather_df, test_weather_df, temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################### Trick to use kernel hdd to store results\n",
    "#################################################################################\n",
    "\n",
    "# You can save just test_df or both if have sufficient space\n",
    "train_df.to_pickle('../output/ashrae-baseline-lgbm/train_df.pkl')\n",
    "test_df.to_pickle('../output/ashrae-baseline-lgbm/test_df.pkl')\n",
    "   \n",
    "del train_df, test_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           _ii:   817.0B\n",
      "                          _i12:   817.0B\n",
      "                          _i21:   817.0B\n",
      "                           _i2:   715.0B\n",
      "                           _i8:   715.0B\n",
      "                          _i17:   715.0B\n",
      "                          _iii:   704.0B\n",
      "                          _i11:   704.0B\n",
      "                          _i20:   704.0B\n",
      "                           _i5:   602.0B\n",
      "Memory in Gb 0.09\n"
     ]
    }
   ],
   "source": [
    "########################### Check memory usage\n",
    "#################################################################################\n",
    "for name, size in sorted(((name, sys.getsizeof(value)) for name,value in locals().items()),\n",
    "                         key= lambda x: -x[1])[:10]:\n",
    "    print(\"{:>30}: {:>8}\".format(name,sizeof_fmt(size)))\n",
    "print('Memory in Gb', get_memory_usage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Model params\n",
    "import lightgbm as lgb\n",
    "lgb_params = {\n",
    "                    'objective':'regression',\n",
    "                    'boosting_type':'gbdt',\n",
    "                    'metric':'rmse',\n",
    "                    'n_jobs':-1,\n",
    "                    'learning_rate':0.05,\n",
    "                    'num_leaves': 2**8,\n",
    "                    'max_depth':-1,\n",
    "                    'tree_learner':'serial',\n",
    "                    'colsample_bytree': 0.7,\n",
    "                    'subsample_freq':1,\n",
    "                    'subsample':0.7,\n",
    "                    'n_estimators':800,\n",
    "                    'max_bin':255,\n",
    "                    'verbose':-1,\n",
    "                    'seed': SEED,\n",
    "                    'early_stopping_rounds':100, \n",
    "                } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.00533\n",
      "[200]\ttraining's rmse: 0.862797\n",
      "[300]\ttraining's rmse: 0.799672\n",
      "[400]\ttraining's rmse: 0.757996\n",
      "[500]\ttraining's rmse: 0.730181\n",
      "[600]\ttraining's rmse: 0.704417\n",
      "[700]\ttraining's rmse: 0.682753\n",
      "[800]\ttraining's rmse: 0.667343\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[800]\ttraining's rmse: 0.667343\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.99445\n",
      "[200]\ttraining's rmse: 0.85574\n",
      "[300]\ttraining's rmse: 0.795205\n",
      "[400]\ttraining's rmse: 0.759439\n",
      "[500]\ttraining's rmse: 0.733553\n",
      "[600]\ttraining's rmse: 0.710513\n",
      "[700]\ttraining's rmse: 0.692225\n",
      "[800]\ttraining's rmse: 0.672614\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[800]\ttraining's rmse: 0.672614\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.993074\n",
      "[200]\ttraining's rmse: 0.857916\n",
      "[300]\ttraining's rmse: 0.795438\n",
      "[400]\ttraining's rmse: 0.75428\n",
      "[500]\ttraining's rmse: 0.725882\n",
      "[600]\ttraining's rmse: 0.703903\n",
      "[700]\ttraining's rmse: 0.685153\n",
      "[800]\ttraining's rmse: 0.667915\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[800]\ttraining's rmse: 0.667915\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.998767\n",
      "[200]\ttraining's rmse: 0.857047\n",
      "[300]\ttraining's rmse: 0.797943\n",
      "[400]\ttraining's rmse: 0.757962\n",
      "[500]\ttraining's rmse: 0.725843\n",
      "[600]\ttraining's rmse: 0.702132\n",
      "[700]\ttraining's rmse: 0.684406\n",
      "[800]\ttraining's rmse: 0.670531\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[800]\ttraining's rmse: 0.670531\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.00004\n",
      "[200]\ttraining's rmse: 0.867418\n",
      "[300]\ttraining's rmse: 0.801791\n",
      "[400]\ttraining's rmse: 0.765903\n",
      "[500]\ttraining's rmse: 0.735431\n",
      "[600]\ttraining's rmse: 0.708957\n",
      "[700]\ttraining's rmse: 0.689061\n",
      "[800]\ttraining's rmse: 0.66856\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[800]\ttraining's rmse: 0.66856\n"
     ]
    }
   ],
   "source": [
    "########################### Model\n",
    "\n",
    "# Models saving\n",
    "model_filename = 'lgbm'\n",
    "models = []\n",
    "\n",
    "# Load train_df from hdd\n",
    "train_df = pd.read_pickle('../output/ashrae-baseline-lgbm/train_df.pkl')\n",
    "\n",
    "remove_columns = ['timestamp',TARGET]\n",
    "features_columns = [col for col in list(train_df) if col not in remove_columns]\n",
    "\n",
    "if LOCAl_TEST:\n",
    "    tr_data = lgb.Dataset(train_df.iloc[:15000000][features_columns], label=np.log1p(train_df.iloc[:15000000][TARGET]))\n",
    "    vl_data = lgb.Dataset(train_df.iloc[15000000:][features_columns], label=np.log1p(train_df.iloc[15000000:][TARGET]))\n",
    "    eval_sets = [tr_data,vl_data]\n",
    "else:\n",
    "    tr_data = lgb.Dataset(train_df[features_columns], label=np.log1p(train_df[TARGET]))\n",
    "    eval_sets = [tr_data]\n",
    "\n",
    "# Remove train_df from hdd\n",
    "#os.system('rm ../output/ashrae-baseline-lgbm/train_df.pkl')\n",
    "\n",
    "# Lets make 5 seeds mix model\n",
    "for cur_seed in [42,43,44,45,46]:\n",
    "    \n",
    "    # Seed everything\n",
    "    seed_everything(cur_seed)\n",
    "    lgb_params['seed'] = cur_seed\n",
    "    \n",
    "    estimator = lgb.train(\n",
    "                lgb_params,\n",
    "                tr_data,\n",
    "                valid_sets = eval_sets,\n",
    "                verbose_eval = 100,\n",
    "            )\n",
    "\n",
    "    # For CV you may add fold number\n",
    "    # pickle.dump(estimator, open(model_filename + '__fold_' + str(i) + '.bin', \"wb\"))\n",
    "    pickle.dump(estimator, open(model_filename + '__seed_' + str(cur_seed)  + '.bin', 'wb'))\n",
    "    models.append(model_filename + '__seed_' + str(cur_seed)  + '.bin')\n",
    "\n",
    "if not LOCAl_TEST:\n",
    "    del tr_data, train_df\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for lgbm__seed_42.bin\n",
      "Predicting batch: 0\n",
      "Predicting batch: 1\n",
      "Predicting batch: 2\n",
      "Predicting batch: 3\n",
      "Predicting batch: 4\n",
      "Predicting batch: 5\n",
      "Predicting batch: 6\n",
      "Predicting batch: 7\n",
      "Predicting batch: 8\n",
      "Predicting batch: 9\n",
      "Predicting batch: 10\n",
      "Predicting batch: 11\n",
      "Predicting batch: 12\n",
      "Predicting batch: 13\n",
      "Predicting batch: 14\n",
      "Predicting batch: 15\n",
      "Predicting batch: 16\n",
      "Predicting batch: 17\n",
      "Predicting batch: 18\n",
      "Predicting batch: 19\n",
      "Predicting batch: 20\n",
      "Predictions for lgbm__seed_43.bin\n",
      "Predicting batch: 0\n",
      "Predicting batch: 1\n",
      "Predicting batch: 2\n",
      "Predicting batch: 3\n",
      "Predicting batch: 4\n",
      "Predicting batch: 5\n",
      "Predicting batch: 6\n",
      "Predicting batch: 7\n",
      "Predicting batch: 8\n",
      "Predicting batch: 9\n",
      "Predicting batch: 10\n",
      "Predicting batch: 11\n",
      "Predicting batch: 12\n",
      "Predicting batch: 13\n",
      "Predicting batch: 14\n",
      "Predicting batch: 15\n",
      "Predicting batch: 16\n",
      "Predicting batch: 17\n",
      "Predicting batch: 18\n",
      "Predicting batch: 19\n",
      "Predicting batch: 20\n",
      "Predictions for lgbm__seed_44.bin\n",
      "Predicting batch: 0\n",
      "Predicting batch: 1\n",
      "Predicting batch: 2\n",
      "Predicting batch: 3\n",
      "Predicting batch: 4\n",
      "Predicting batch: 5\n",
      "Predicting batch: 6\n",
      "Predicting batch: 7\n",
      "Predicting batch: 8\n",
      "Predicting batch: 9\n",
      "Predicting batch: 10\n",
      "Predicting batch: 11\n",
      "Predicting batch: 12\n",
      "Predicting batch: 13\n",
      "Predicting batch: 14\n",
      "Predicting batch: 15\n",
      "Predicting batch: 16\n",
      "Predicting batch: 17\n",
      "Predicting batch: 18\n",
      "Predicting batch: 19\n",
      "Predicting batch: 20\n",
      "Predictions for lgbm__seed_45.bin\n",
      "Predicting batch: 0\n",
      "Predicting batch: 1\n",
      "Predicting batch: 2\n",
      "Predicting batch: 3\n",
      "Predicting batch: 4\n",
      "Predicting batch: 5\n",
      "Predicting batch: 6\n",
      "Predicting batch: 7\n",
      "Predicting batch: 8\n",
      "Predicting batch: 9\n",
      "Predicting batch: 10\n",
      "Predicting batch: 11\n",
      "Predicting batch: 12\n",
      "Predicting batch: 13\n",
      "Predicting batch: 14\n",
      "Predicting batch: 15\n",
      "Predicting batch: 16\n",
      "Predicting batch: 17\n",
      "Predicting batch: 18\n",
      "Predicting batch: 19\n",
      "Predicting batch: 20\n",
      "Predictions for lgbm__seed_46.bin\n",
      "Predicting batch: 0\n",
      "Predicting batch: 1\n",
      "Predicting batch: 2\n",
      "Predicting batch: 3\n",
      "Predicting batch: 4\n",
      "Predicting batch: 5\n",
      "Predicting batch: 6\n",
      "Predicting batch: 7\n",
      "Predicting batch: 8\n",
      "Predicting batch: 9\n",
      "Predicting batch: 10\n",
      "Predicting batch: 11\n",
      "Predicting batch: 12\n",
      "Predicting batch: 13\n",
      "Predicting batch: 14\n",
      "Predicting batch: 15\n",
      "Predicting batch: 16\n",
      "Predicting batch: 17\n",
      "Predicting batch: 18\n",
      "Predicting batch: 19\n",
      "Predicting batch: 20\n",
      "    meter_reading  row_id\n",
      "0        0.726651       0\n",
      "1        0.628281       1\n",
      "2        0.000000       2\n",
      "3        1.097136       3\n",
      "4        1.271202       4\n",
      "5        0.075806       5\n",
      "6        0.680700       6\n",
      "7        1.114888       7\n",
      "8      133.250716       8\n",
      "9        0.669064       9\n",
      "10       0.513313      10\n",
      "11       2.178356      11\n",
      "12       2.479651      12\n",
      "13       1.084839      13\n",
      "14       0.782074      14\n",
      "15       0.661250      15\n",
      "16       8.505401      16\n",
      "17       0.674296      17\n",
      "18      66.964134      18\n",
      "19       1.253895      19\n",
      "count    4.169760e+07\n",
      "mean     8.622321e+02\n",
      "std      5.122801e+04\n",
      "min      0.000000e+00\n",
      "25%      1.878386e+01\n",
      "50%      7.085340e+01\n",
      "75%      2.224288e+02\n",
      "max      1.269679e+07\n",
      "Name: meter_reading, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "########################### Predict\n",
    "#################################################################################\n",
    "if not LOCAl_TEST:\n",
    "    \n",
    "    # Load test_df from hdd\n",
    "    test_df = pd.read_pickle('../output/ashrae-baseline-lgbm/test_df.pkl')\n",
    "    \n",
    "    # Remove unused columns\n",
    "    test_df = test_df[features_columns]\n",
    "    \n",
    "    # Remove test_df from hdd\n",
    "    #os.system('rm ../output/ashrae-baseline-lgbm/test_df.pkl')\n",
    "    \n",
    "    # Read submission file\n",
    "    submission = pd.read_csv('../input/ashrae-energy-prediction/sample_submission.csv')\n",
    "\n",
    "    # Remove row_id for a while\n",
    "    del submission['row_id']\n",
    "    \n",
    "    for model_path in models:\n",
    "        print('Predictions for', model_path)\n",
    "        estimator = pickle.load(open(model_path, 'rb'))\n",
    "\n",
    "        predictions = []\n",
    "        batch_size = 2000000\n",
    "        for batch in range(int(len(test_df)/batch_size)+1):\n",
    "            print('Predicting batch:', batch)\n",
    "            predictions += list(np.expm1(estimator.predict(test_df[features_columns].iloc[batch*batch_size:(batch+1)*batch_size])))\n",
    "            \n",
    "        submission['meter_reading'] += predictions\n",
    "        \n",
    "    # Average over models\n",
    "    submission['meter_reading'] /= len(models)\n",
    "    \n",
    "    # Delete test_df\n",
    "    del test_df\n",
    "     \n",
    "    # Fix negative values\n",
    "    submission['meter_reading'] = submission['meter_reading'].clip(0,None)\n",
    "\n",
    "    # Restore row_id\n",
    "    submission['row_id'] = submission.index\n",
    "    \n",
    "    ########################### Check\n",
    "    print(submission.iloc[:20])\n",
    "    print(submission['meter_reading'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Export\n",
    "#################################################################################\n",
    "if not LOCAl_TEST:\n",
    "    submission.to_csv('../output/ashrae-baseline-lgbm/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
