{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, gc, sys, warnings, random, math, psutil, pickle\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Helpers\n",
    "#################################################################################\n",
    "## Seeder\n",
    "# :seed to make all processes deterministic     # type: int\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "## Simple \"Memory profilers\" to see memory usage\n",
    "def get_memory_usage():\n",
    "    return np.round(psutil.Process(os.getpid()).memory_info()[0]/2.**30, 2) \n",
    "        \n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "## Memory Reducer\n",
    "# :df pandas dataframe to reduce size             # type: pd.DataFrame()\n",
    "# :verbose                                        # type: bool\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        if col!=TARGET:\n",
    "            col_type = df[col].dtypes\n",
    "            if col_type in numerics:\n",
    "                c_min = df[col].min()\n",
    "                c_max = df[col].max()\n",
    "                if str(col_type)[:3] == 'int':\n",
    "                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                        df[col] = df[col].astype(np.int8)\n",
    "                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                        df[col] = df[col].astype(np.int16)\n",
    "                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                        df[col] = df[col].astype(np.int32)\n",
    "                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                        df[col] = df[col].astype(np.int64)  \n",
    "                else:\n",
    "                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                        df[col] = df[col].astype(np.float16)\n",
    "                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                        df[col] = df[col].astype(np.float32)\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Vars\n",
    "#################################################################################\n",
    "SEED = 42\n",
    "LOCAl_TEST = False\n",
    "seed_everything(SEED)\n",
    "TARGET = 'meter_reading'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from os.path import join as pjoin\n",
    "# RAW_DATA_DIR = '/kaggle/input/ashrae-energy-prediction/'\n",
    "\n",
    "# weather_dtypes = {\n",
    "#     'site_id': np.uint8,\n",
    "#     'air_temperature': np.float32,\n",
    "#     'cloud_coverage': np.float32,\n",
    "#     'dew_temperature': np.float32,\n",
    "#     'precip_depth_1_hr': np.float32,\n",
    "#     'sea_level_pressure': np.float32,\n",
    "#     'wind_direction': np.float32,\n",
    "#     'wind_speed': np.float32,\n",
    "# }\n",
    "\n",
    "# weather_train = pd.read_csv(pjoin(RAW_DATA_DIR, 'weather_train.csv'),dtype=weather_dtypes,\n",
    "#     parse_dates=['timestamp'])\n",
    "# weather_test = pd.read_csv(pjoin(RAW_DATA_DIR, 'weather_test.csv'),dtype=weather_dtypes,\n",
    "#     parse_dates=['timestamp'])\n",
    "\n",
    "# weather = pd.concat([weather_train,weather_test],ignore_index=True)\n",
    "# del weather_train, weather_test\n",
    "# weather_key = ['site_id', 'timestamp']\n",
    "# temp_skeleton = weather[weather_key + ['air_temperature']].drop_duplicates(subset=weather_key).sort_values(by=weather_key).copy()\n",
    "# del weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # calculate ranks of hourly temperatures within date/site_id chunks\n",
    "# temp_skeleton['temp_rank'] = temp_skeleton.groupby(['site_id', temp_skeleton.timestamp.dt.date])['air_temperature'].rank('average')\n",
    "\n",
    "# # create a dataframe of site_ids (0-16) x mean hour rank of temperature within day (0-23)\n",
    "# df_2d = temp_skeleton.groupby(['site_id', temp_skeleton.timestamp.dt.hour])['temp_rank'].mean().unstack(level=1)\n",
    "\n",
    "# # Subtract the columnID of temperature peak by 14, getting the timestamp alignment gap.\n",
    "# site_ids_offsets = pd.Series(df_2d.values.argmax(axis=1) - 14)\n",
    "# site_ids_offsets.index.name = 'site_id'\n",
    "\n",
    "# def timestamp_align(df):\n",
    "#     df['offset'] = df.site_id.map(site_ids_offsets)\n",
    "#     df['timestamp_aligned'] = (df.timestamp - pd.to_timedelta(df.offset, unit='H'))\n",
    "#     df['timestamp'] = df['timestamp_aligned']\n",
    "#     del df['timestamp_aligned']\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########################### DATA LOAD\n",
    "# #################################################################################\n",
    "# print('Load Data')\n",
    "# train_df = pd.read_pickle('../input/as-data-minification/train.pkl')\n",
    "# test_df = pd.read_pickle('../input/as-data-minification/test.pkl')\n",
    "\n",
    "# building_df = pd.read_pickle('../input/as-data-minification/building_metadata.pkl')\n",
    "\n",
    "# train_weather_df = pd.read_pickle('../input/as-data-minification/weather_train.pkl')\n",
    "# test_weather_df = pd.read_pickle('../input/as-data-minification/weather_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_weather_df = timestamp_align(train_weather_df)\n",
    "# test_weather_df = timestamp_align(test_weather_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # 单独插值，变弱\n",
    "# # train_weather_df = train_weather_df.groupby('site_id').apply(lambda group: group.interpolate(limit_direction='both'))\n",
    "\n",
    "# def add_lag_feature(weather_df, window=3):\n",
    "#     group_df = weather_df.groupby('site_id')\n",
    "#     cols = ['air_temperature', 'cloud_coverage', 'dew_temperature', 'precip_depth_1_hr', 'sea_level_pressure', 'wind_direction', 'wind_speed']\n",
    "#     rolled = group_df[cols].rolling(window=window, min_periods=0)\n",
    "#     lag_mean = rolled.mean().reset_index().astype(np.float16)\n",
    "#     lag_max = rolled.max().reset_index().astype(np.float16)\n",
    "#     lag_min = rolled.min().reset_index().astype(np.float16)\n",
    "#     lag_std = rolled.std().reset_index().astype(np.float16)\n",
    "#     lag_median = rolled.median().reset_index().astype(np.float16)\n",
    "#     lag_skew = rolled.skew().reset_index().astype(np.float16)\n",
    "#     for col in cols:\n",
    "#         weather_df[f'{col}_mean_lag{window}'] = lag_mean[col]\n",
    "#         weather_df[f'{col}_max_lag{window}'] = lag_max[col]\n",
    "#         weather_df[f'{col}_min_lag{window}'] = lag_min[col]\n",
    "#         weather_df[f'{col}_std_lag{window}'] = lag_std[col]\n",
    "#         weather_df[f'{col}_median_lag{window}'] = lag_median[col]\n",
    "#         weather_df[f'{col}_skew_lag{window}'] = lag_skew[col]\n",
    "\n",
    "# add_lag_feature(train_weather_df, window=3)\n",
    "# # add_lag_feature(train_weather_df, window=72)\n",
    "\n",
    "# add_lag_feature(train_weather_df, window=18)\n",
    "\n",
    "# add_lag_feature(test_weather_df, window=3)\n",
    "# # add_lag_feature(test_weather_df, window=72)\n",
    "\n",
    "# add_lag_feature(test_weather_df, window=18)\n",
    "\n",
    "# # add_lag_feature(train_weather_df, window=24)\n",
    "# # add_lag_feature(train_weather_df, window=168)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather_col = ['site_id', 'timestamp', 'air_temperature', 'cloud_coverage', 'dew_temperature', 'precip_depth_1_hr', 'sea_level_pressure', 'wind_direction', 'wind_speed', 'offset',\n",
    "#               'air_temperature_min_lag3','air_temperature_min_lag18','air_temperature_std_lag18','cloud_coverage_median_lag18']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_weather_df = train_weather_df[weather_col]\n",
    "# test_weather_df = test_weather_df[weather_col]\n",
    "\n",
    "# # train_weather_df = reduce_mem_usage(train_weather_df)\n",
    "# # test_weather_df = reduce_mem_usage(test_weather_df)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########################### Building DF merge through concat \n",
    "# #################################################################################\n",
    "# # Benefits of concat:\n",
    "# ## Faster for huge datasets (columns number)\n",
    "# ## No dtype change for dataset\n",
    "# ## Consume less memmory \n",
    "\n",
    "# temp_df = train_df[['building_id']]\n",
    "# temp_df = temp_df.merge(building_df, on=['building_id'], how='left')\n",
    "# del temp_df['building_id']\n",
    "# train_df = pd.concat([train_df, temp_df], axis=1)\n",
    "\n",
    "# temp_df = test_df[['building_id']]\n",
    "# temp_df = temp_df.merge(building_df, on=['building_id'], how='left')\n",
    "# del temp_df['building_id']\n",
    "# test_df = pd.concat([test_df, temp_df], axis=1)\n",
    "\n",
    "# del building_df, temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########################### Weather DF merge over concat (to not lose type)\n",
    "# #################################################################################\n",
    "# # Benefits of concat:\n",
    "# ## Faster for huge datasets (columns number)\n",
    "# ## No dtype change for dataset\n",
    "# ## Consume less memmory \n",
    "\n",
    "# temp_df = train_df[['site_id','timestamp']]\n",
    "# temp_df = temp_df.merge(train_weather_df, on=['site_id','timestamp'], how='left')\n",
    "# del temp_df['site_id'], temp_df['timestamp']\n",
    "# train_df = pd.concat([train_df, temp_df], axis=1)\n",
    "\n",
    "# temp_df = test_df[['site_id','timestamp']]\n",
    "# temp_df = temp_df.merge(test_weather_df, on=['site_id','timestamp'], how='left')\n",
    "# del temp_df['site_id'], temp_df['timestamp']\n",
    "# test_df = pd.concat([test_df, temp_df], axis=1)\n",
    "\n",
    "# del train_weather_df, test_weather_df, temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Building and site id and primary_use\n",
    "# for enc_col in ['site_id']:\n",
    "#     # 该操作有重复\n",
    "#     temp_df = train_df.groupby([enc_col])['meter'].agg(['unique'])\n",
    "#     temp_df['unique'] = temp_df['unique'].apply(lambda x: '_'.join(str(x))).astype(str)\n",
    "\n",
    "#     le = LabelEncoder()\n",
    "#     temp_df['unique'] = le.fit_transform(temp_df['unique']).astype(np.int8)\n",
    "#     temp_df = temp_df['unique'].to_dict()\n",
    "\n",
    "#     train_df[enc_col+'_uid_enc'] = train_df[enc_col].map(temp_df)\n",
    "#     test_df[enc_col+'_uid_enc'] = test_df[enc_col].map(temp_df)\n",
    "# del temp_df\n",
    "\n",
    "# for enc_col in ['site_id_uid_enc']:\n",
    "#     train_df['g_meter_'+enc_col] = train_df['meter'].astype(str) +'_'+\\\n",
    "#                     train_df[enc_col].astype(str)\n",
    "\n",
    "#     test_df['g_meter_'+enc_col] = test_df['meter'].astype(str) +'_'+\\\n",
    "#                         test_df[enc_col].astype(str)\n",
    "\n",
    "#     le = LabelEncoder()\n",
    "#     train_df['g_meter_'+enc_col] = le.fit_transform(train_df['g_meter_'+enc_col]).astype(np.int8)\n",
    "#     test_df['g_meter_'+enc_col] = le.fit_transform(test_df['g_meter_'+enc_col]).astype(np.int8)\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['meter_reading_log1p'] = np.log1p(train_df['meter'])\n",
    "# df_group = train_df.groupby('building_id')['meter_reading_log1p']\n",
    "# # building_mean = df_group.mean().astype(np.float16)\n",
    "# building_median = df_group.median().astype(np.float16)\n",
    "# building_min = df_group.min().astype(np.float16)\n",
    "# # building_max = df_group.max().astype(np.float16)\n",
    "# # building_std = df_group.std().astype(np.float16)\n",
    "\n",
    "# # train_df['building_mean'] = train_df['building_id'].map(building_mean)\n",
    "# train_df['building_median'] = train_df['building_id'].map(building_median)\n",
    "# train_df['building_min'] = train_df['building_id'].map(building_min)\n",
    "# # train_df['building_max'] = train_df['building_id'].map(building_max)\n",
    "# # train_df['building_std'] = train_df['building_id'].map(building_std)\n",
    "\n",
    "# # test_df['building_mean'] = test_df['building_id'].map(building_mean)\n",
    "# test_df['building_median'] = test_df['building_id'].map(building_median)\n",
    "# test_df['building_min'] = test_df['building_id'].map(building_min)\n",
    "# # test_df['building_max'] = test_df['building_id'].map(building_max)\n",
    "# # test_df['building_std'] = test_df['building_id'].map(building_std)\n",
    "\n",
    "# i_cols = [\n",
    "#          'meter_reading_log1p',\n",
    "#         ]\n",
    "\n",
    "# for col in i_cols:\n",
    "#     del train_df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 变强啦\n",
    "# i_cols = [\n",
    "#         'building_id',\n",
    "#         'site_id',\n",
    "#         'primary_use',\n",
    "# #         'DT_M',\n",
    "#         'floor_count',\n",
    "# #         'building_id_uid_enc', \n",
    "# #         'site_id_uid_enc',\n",
    "#         'g_meter_site_id_uid_enc'\n",
    "# ]\n",
    "\n",
    "# for col in i_cols:\n",
    "#     train_df[col] = train_df[col].astype('category')\n",
    "#     test_df[col] = test_df[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del train_df\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models saving\n",
    "model_filename = 'xgb'\n",
    "\n",
    "test_path = os.path.join('..', 'output', 'fork-of-as-2kfold-model6-test-df')\n",
    "\n",
    "# Load train_df from hdd\n",
    "test_df = pd.read_pickle(os.path.join(train_path, 'test_df.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       test_df:   3.9GiB\n",
      "                           _i8:   3.2KiB\n",
      "                           _i2:   2.5KiB\n",
      "                          _i13:   2.2KiB\n",
      "                          _i14:   1.3KiB\n",
      "                  LabelEncoder:   1.0KiB\n",
      "                           _i4:   996.0B\n",
      "                           _i5:   924.0B\n",
      "                          _i12:   847.0B\n",
      "                          _iii:   820.0B\n",
      "Memory in Gb 4.2\n"
     ]
    }
   ],
   "source": [
    "########################### Check memory usage\n",
    "#################################################################################\n",
    "for name, size in sorted(((name, sys.getsizeof(value)) for name,value in locals().items()),\n",
    "                         key= lambda x: -x[1])[:10]:\n",
    "    print(\"{:>30}: {:>8}\".format(name,sizeof_fmt(size)))\n",
    "print('Memory in Gb', get_memory_usage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for kfold_1\n",
      "Predicting batch: 0\n",
      "Predicting batch: 1\n",
      "Predicting batch: 2\n",
      "Predicting batch: 3\n",
      "Predicting batch: 4\n",
      "Predicting batch: 5\n",
      "Predicting batch: 6\n",
      "Predicting batch: 7\n",
      "Predicting batch: 8\n",
      "Predicting batch: 9\n",
      "Predicting batch: 10\n",
      "Predicting batch: 11\n",
      "Predicting batch: 12\n",
      "Predicting batch: 13\n",
      "Predicting batch: 14\n",
      "Predicting batch: 15\n",
      "Predicting batch: 16\n",
      "Predicting batch: 17\n",
      "Predicting batch: 18\n",
      "Predicting batch: 19\n",
      "Predicting batch: 20\n",
      "Predictions for kfold_2\n",
      "Predicting batch: 0\n",
      "Predicting batch: 1\n",
      "Predicting batch: 2\n",
      "Predicting batch: 3\n",
      "Predicting batch: 4\n",
      "Predicting batch: 5\n",
      "Predicting batch: 6\n",
      "Predicting batch: 7\n",
      "Predicting batch: 8\n",
      "Predicting batch: 9\n",
      "Predicting batch: 10\n",
      "Predicting batch: 11\n",
      "Predicting batch: 12\n",
      "Predicting batch: 13\n",
      "Predicting batch: 14\n",
      "Predicting batch: 15\n",
      "Predicting batch: 16\n",
      "Predicting batch: 17\n",
      "Predicting batch: 18\n",
      "Predicting batch: 19\n",
      "Predicting batch: 20\n",
      "    meter_reading  row_id\n",
      "0      137.287292       0\n",
      "1       67.444572       1\n",
      "2        7.209320       2\n",
      "3      242.232513       3\n",
      "4     1145.151978       4\n",
      "5       13.416656       5\n",
      "6      101.864433       6\n",
      "7      486.159668       7\n",
      "8      740.846741       8\n",
      "9      309.820892       9\n",
      "10      52.455444      10\n",
      "11      10.559954      11\n",
      "12     950.796509      12\n",
      "13     351.308533      13\n",
      "14     188.599136      14\n",
      "15     189.333008      15\n",
      "16     136.788940      16\n",
      "17     258.451660      17\n",
      "18     463.836365      18\n",
      "19     195.225952      19\n",
      "count    4.169760e+07\n",
      "mean     3.323522e+02\n",
      "std      1.754512e+03\n",
      "min      0.000000e+00\n",
      "25%      2.205619e+01\n",
      "50%      7.717932e+01\n",
      "75%      2.361923e+02\n",
      "max      1.109689e+05\n",
      "Name: meter_reading, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "########################### Predict\n",
    "#################################################################################\n",
    "if not LOCAl_TEST:\n",
    "    \n",
    "    # Load test_df from hdd\n",
    "#     test_df = pd.read_pickle('test_df.pkl')\n",
    "\n",
    "    remove_columns = ['timestamp',TARGET,'row_id','DT_M']\n",
    "    features_columns = [col for col in list(test_df) if col not in remove_columns]\n",
    "    \n",
    "    # CV1最终\n",
    "    features_columns = ['square_feet','building_id','meter','air_temperature','year_built','primary_use','floor_count','dew_temperature','DT_hour','sea_level_pressure','DT_day_week','site_id',\n",
    "                       'DT_day_month','cloud_coverage','wind_speed','wind_direction']\n",
    "    \n",
    "    # 添加g_meter_site_id_uid_enc，超过cv2\n",
    "    features_columns += ['g_meter_site_id_uid_enc']\n",
    "    \n",
    "    # 都强点\n",
    "    features_columns += ['building_median']\n",
    "\n",
    "    # 验证强点，测试强多\n",
    "    features_columns += ['building_min']\n",
    "\n",
    "    # 强一丢丢\n",
    "    features_columns += ['air_temperature_min_lag3']\n",
    "\n",
    "    # 都强\n",
    "    features_columns += ['air_temperature_min_lag18']\n",
    "\n",
    "    # # 都强\n",
    "    features_columns += ['air_temperature_std_lag18']\n",
    "\n",
    "    # 都强\n",
    "    features_columns += ['cloud_coverage_median_lag18']\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Remove unused columns\n",
    "    test_df = test_df[features_columns]\n",
    "    gc.collect()\n",
    "    \n",
    "    # Remove test_df from hdd\n",
    "#     os.system('rm test_df.pkl')\n",
    "    \n",
    "    # Read submission file\n",
    "    submission = pd.read_csv(os.path.join('..', 'input', 'ashrae-energy-prediction', 'sample_submission.csv'))\n",
    "\n",
    "    # Remove row_id for a while\n",
    "    del submission['row_id']\n",
    "    \n",
    "    for i in range(1, 3):\n",
    "        print(f'Predictions for kfold_{i}')\n",
    "        if i == 1:\n",
    "#             estimator = xgb.Booster(model_file='../input/fork-of-as-2kfold-model6-xgb-fold0/xgb_kfold_1.model')\n",
    "            model_path = os.path.join('..', 'output', 'fork-of-as-2kfold-model6-xgb-fr7d12-fold0', 'xgb_kfold_1.bin')\n",
    "            estimator = pickle.load(open(model_path, 'rb'))\n",
    "        elif i == 2:\n",
    "#             estimator = xgb.Booster(model_file='../input/fork-of-as-2kfold-model6-xgb-fold1/xgb_kfold_2.model')\n",
    "            model_path = os.path.join('..', 'output', 'fork-of-as-2kfold-model6-xgb-fr7d12-fold1', 'xgb_kfold_2.bin')\n",
    "            estimator = pickle.load(open(model_path, 'rb'))\n",
    "\n",
    "        predictions = []\n",
    "        batch_size = 2000000\n",
    "        for batch in range(int(len(test_df)/batch_size)+1):\n",
    "            print('Predicting batch:', batch)\n",
    "#             predictions += list(np.expm1(estimator.predict(xgb.DMatrix(test_df[features_columns].iloc[batch*batch_size:(batch+1)*batch_size]))))\n",
    "            predictions += list(np.expm1(estimator.predict(test_df[features_columns].iloc[batch*batch_size:(batch+1)*batch_size])))\n",
    "            \n",
    "        submission['meter_reading'] += predictions\n",
    "        del estimator\n",
    "        gc.collect()\n",
    "        \n",
    "    # Average over models\n",
    "    submission['meter_reading'] /= 2\n",
    "    \n",
    "    # Delete test_df\n",
    "    del test_df\n",
    "     \n",
    "    # Fix negative values\n",
    "    submission['meter_reading'] = submission['meter_reading'].clip(0,None)\n",
    "\n",
    "    # Restore row_id\n",
    "    submission['row_id'] = submission.index\n",
    "    \n",
    "    ########################### Check\n",
    "    print(submission.iloc[:20])\n",
    "    print(submission['meter_reading'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Export\n",
    "#################################################################################\n",
    "if not LOCAl_TEST:\n",
    "    output_path = os.path.join('..', 'output', 'fork-of-as-2kfold-model6-xgb-fr7d12-pred')\n",
    "    submission.to_csv(os.path.join(output_path, 'submission.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
